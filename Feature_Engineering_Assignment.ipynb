{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature** **Engineering** **Assignment**"
      ],
      "metadata": {
        "id": "p5Bp4uIA0K97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.1 What is a parameter?\n",
        "\n",
        "   -> In machine learning, parameters are internal variables within a model\n",
        "      that the model learns from the training data. These parameters are adjusted during training to optimize the model's performance.\n",
        "      \n",
        "      A parameter is a characteristic that describes a population.\n",
        "      It's a numerical value that quantifies a specific aspect of the\n",
        "      entire group. Parameters are often unknown and difficult to obtain,\n",
        "      but can be estimated using sample statistics.\n",
        "\n",
        "      A parameter is a number that describes a characteristic of a\n",
        "      population, such as the population mean or the population standard deviation.\n",
        "\n",
        "      Parameters are fundamental in data science because they\n",
        "      allow researchers to understand and draw conclusions about\n",
        "      the characteristics of a population.\n",
        "      \n",
        "      Key Characteristics:\n",
        "      1.Parameters describe the entire population.\n",
        "      2.They are usually unknown and require estimation.\n",
        "      3.They are essential for making generalizations about a population\n",
        "        based on sample data.\n"
      ],
      "metadata": {
        "id": "kiG86zqQjcCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.2 What is correlation?\n",
        "\n",
        "  ->  Correlation is a statistical measure that describes the\n",
        "      strength and direction of the relationship between two variables.\n",
        "\n",
        "      Types of Correlation:\n",
        "      1.Positive Correlation: Variables tend to increase or decrease together.\n",
        "      2.Negative Correlation: One variable tends to increase as the other\n",
        "        decreases.\n",
        "      3.No Correlation: No discernible relationship.\n",
        "\n"
      ],
      "metadata": {
        "id": "-j2yo3smjk2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "     What does negative correlation mean?\n",
        "\n",
        " ->  A negative correlation, also known as an inverse correlation, means that  \n",
        "     as one variable increases, the other variable decreases, and vice versa.\n",
        "\n",
        "     Example:\n",
        "     A classic example is the relationship between hours worked and\n",
        "     leisure time. As hours worked increase, leisure time tends to\n",
        "     decrease, illustrating a negative correlation."
      ],
      "metadata": {
        "id": "qf6JweJXjsqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.3 Define Machine Learning. What are the main components in Machine\n",
        "      Learning?\n",
        "\n",
        "  ->  Machine Learning (ML) is a branch of artificial intelligence that  \n",
        "      focuses on enabling computers to learn from data without being explicitly programmed. It involves using algorithms to analyze data, identify patterns, and make predictions or decisions.\n",
        "\n",
        "      The main components of Machine Learning include:\n",
        "      1. Algorithms:\n",
        "         These are sets of instructions or rules that allow computers to\n",
        "         learn from data and make predictions.\n",
        "      2. Data:\n",
        "         This is the input that the algorithms use to learn and\n",
        "         make predictions. The data can be structured or unstructured.\n",
        "      3. Models:\n",
        "         These are the representations or structures that the algorithms\n",
        "         learn from the data. They capture the relationships and patterns\n",
        "         in the data.\n",
        "      4. Predictions:\n",
        "         These are the outputs or results that the models make based on\n",
        "         the learned patterns and the new input data.\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "iFvIgM04jwvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.4 How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "  ->  A lower loss value generally indicates a better-performing model, as it\n",
        "      signifies that the model's predictions are closer to the actual or ground truth values. The loss function quantifies the difference between predicted and actual values, and a lower loss indicates a smaller discrepancy, suggesting the model is more accurate. During training, the goal is to minimize the loss, meaning the model's parameters are adjusted to make better predictions.\n",
        "      "
      ],
      "metadata": {
        "id": "nPg12oDmyHXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.5 What are continuous and categorical variables?\n",
        "\n",
        "  ->  Continuous and categorical variables are fundamental concepts in\n",
        "      statistics and data analysis. Continuous variables can take on any value within a given range, while categorical variables represent distinct groups or categories.\n",
        "\n",
        "      Continuous Variables:\n",
        "      Definition:\n",
        "      Continuous variables are numeric and can have an infinite number\n",
        "      of values between any two values.\n",
        "\n",
        "      Examples:\n",
        "      Height, weight, temperature, time, and age are all examples\n",
        "      of continuous variables.\n",
        "\n",
        "      Measurement:\n",
        "      Continuous variables are typically measured using scales like\n",
        "      interval and ratio scales, where the difference between values is meaningful.\n",
        "\n",
        "      Analysis:\n",
        "      Continuous data is often analyzed using descriptive statistics\n",
        "      like mean, median, and standard deviation, or through statistical\n",
        "      tests like t-tests and ANOVA.\n",
        "\n",
        "      Categorical Variables:\n",
        "      Definition:\n",
        "      Categorical variables describe qualities or characteristics that\n",
        "      cannot be quantified numerically.\n",
        "\n",
        "      Examples:\n",
        "      Gender (male/female), hair color (blonde, brown, black), and\n",
        "      type of transportation (car, bus, train) are examples of\n",
        "      categorical variables.\n",
        "\n",
        "      Measurement:\n",
        "      Categorical variables are usually described using nominal or ordinal scales.\n",
        "\n",
        "      Analysis:\n",
        "      Categorical data is often analyzed using methods like\n",
        "      frequency distributions, chi-square tests, and analysis of\n",
        "      contingency tables."
      ],
      "metadata": {
        "id": "LqLiecINy47e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.6 How do we handle categorical variables in Machine Learning? What are the\n",
        "      common techniques?\n",
        "\n",
        "  ->  Categorical variables in machine learning are handled through encoding\n",
        "      techniques that convert them into numerical representations. Common methods include one-hot encoding, label encoding, ordinal encoding, and target encoding, each with its own strengths and weaknesses.\n",
        "\n",
        "      1. One-Hot Encoding:\n",
        "         Creates a binary (0 or 1) column for each category within a variable.\n",
        "         Good for nominal variables (no inherent order).\n",
        "         Can lead to high dimensionality with many categories.\n",
        "         Often used in conjunction with dropping one dummy variable to\n",
        "         avoid multicollinearity.\n",
        "\n",
        "      2. Label Encoding:\n",
        "         Assigns a unique integer to each category in the order of appearance.\n",
        "         Simpler than one-hot encoding, but doesn't account for category order.\n",
        "\n",
        "      3. Ordinal Encoding:\n",
        "         Used for categorical variables with an inherent order\n",
        "         (e.g., small, medium, large).\n",
        "         Assigns numerical values based on the order, preserving the relationship.\n",
        "         Suitable for models that can interpret magnitude (e.g., linear models).\n",
        "\n",
        "      4. Target Encoding:\n",
        "         Replaces each category with the average value of the target\n",
        "         variable for that category.\n",
        "         Can improve model performance by providing the model with a\n",
        "         numerical representation of the category's relationship to the target.\n",
        "         Requires careful consideration of potential overfitting or\n",
        "         leakage from the target.  "
      ],
      "metadata": {
        "id": "zb6G4kjT0N-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.7 What do you mean by training and testing a dataset?\n",
        "\n",
        "  ->  In machine learning, training a dataset means using a portion of the  \n",
        "      data to teach a model how to make predictions or identify patterns. Testing a dataset means evaluating the model's performance on a separate portion of the data that it hasn't been trained on. This helps ensure the model generalizes well to new, unseen data.\n",
        "\n",
        "      Training Data:\n",
        "      Purpose:\n",
        "      Used to teach the model how to learn by presenting it with\n",
        "      examples (inputs) and their corresponding known outputs.\n",
        "\n",
        "      Process:\n",
        "      The model adjusts its internal parameters (weights, biases) to\n",
        "      minimize prediction errors based on the training data.\n",
        "\n",
        "      Example:\n",
        "      If training a model to identify cats in images, the training data\n",
        "      would consist of images labeled as \"cat\" or \"not cat\".\n",
        "\n",
        "\n",
        "      Testing Data:\n",
        "      Purpose:\n",
        "      Used to assess how well the trained model performs on data it has\n",
        "      never seen before.\n",
        "\n",
        "      Process:\n",
        "      The model makes predictions on the test data, and these predictions\n",
        "      are then compared to the known ground truth to calculate\n",
        "      performance metrics (e.g., accuracy, precision, recall).\n",
        "\n",
        "      Importance:\n",
        "      Ensures the model generalizes well and doesn't just memorize\n",
        "      the training data.\n",
        "\n",
        "      Example:\n",
        "      After training the cat-detection model, the testing data would\n",
        "      consist of a new set of images that the model hasn't seen\n",
        "      during training, and its predictions would be compared to the true\n",
        "      labels to see how well it identifies cats.\n"
      ],
      "metadata": {
        "id": "l5f7jDLs2YIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.8 What is sklearn.preprocessing?\n",
        "\n",
        "  ->  The sklearn.preprocessing module in scikit-learn provides a set of tools\n",
        "      for transforming raw data into a format suitable for machine learning models. It includes functions for scaling, normalizing, encoding, and imputing data. Data preprocessing is a crucial step in the machine learning pipeline because it can significantly impact the performance of the models. Different algorithms have different requirements regarding the input data format, and preprocessing ensures that the data is in a compatible format.\n",
        "\n",
        "      Common preprocessing techniques:\n",
        "      Scaling:\n",
        "      Transforms numerical features to a specific range, such\n",
        "      as between 0 and 1 (MinMaxScaler) or centers the data around 0 with\n",
        "      unit variance (StandardScaler).\n",
        "\n",
        "      Normalization:\n",
        "      Scales individual samples to have unit norm, often used when\n",
        "      the magnitude of the data is not important.\n",
        "\n",
        "      Encoding categorical features:\n",
        "      Converts categorical features into numerical representations,\n",
        "      such as one-hot encoding or label encoding.\n",
        "\n",
        "      Imputation:\n",
        "      Fills in missing values with estimated values, such as\n",
        "      the mean or median.\n",
        "\n",
        "      Non-linear transformations:\n",
        "      Applies non-linear functions to the\n",
        "      data, such as quantile transforms or power transforms, to make the\n",
        "      data more Gaussian-like.\n",
        "\n",
        "      Polynomial features:\n",
        "      Generates polynomial combinations of the features.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFqeMQM43vxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.9 What is a Test set?\n",
        "\n",
        "   -> A \"Test Set\" is a group of test cases, or tests, that are executed\n",
        "      together to assess the functionality or performance of a system or component. These sets can be used for various purposes, such as regression testing, smoke testing, or testing specific features. They can include both manual and automated tests.\n",
        "\n",
        "      Logical Grouping:\n",
        "      Test sets are created by grouping test cases that are related or\n",
        "      have a common purpose.\n",
        "\n",
        "      Execution Context:\n",
        "      They define the tests to be executed within a specific run,\n",
        "      like a regression test suite or a smoke test run.\n",
        "\n",
        "      Test Case Assignment:\n",
        "      Individual test cases can be assigned to multiple test sets,\n",
        "      allowing for flexibility in how tests are organized and executed.\n",
        "\n",
        "      Test Plan Creation:\n",
        "      Test sets can serve as the foundation for creating test plans\n",
        "      or executing specific tests within a larger testing effort.\n",
        "\n",
        "      Diverse Testing Types:\n",
        "      Test sets can contain a mix of automated, manual, BDD\n",
        "      (Behavior-Driven Development), and exploratory tests.\n",
        "\n",
        "      Data Science Context:\n",
        "      In the context of machine learning, a test set is a separate portion\n",
        "      of the dataset used to evaluate the performance of a model after it\n",
        "      has been trained. This is crucial for ensuring the model\n",
        "      generalizes well to new, unseen data and that it is not overfitted\n",
        "      to the training data."
      ],
      "metadata": {
        "id": "6aCpHsPX5r1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.10 How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "   ->  In Python, data is typically split into training and testing sets using\n",
        "       the train_test_split function from the scikit-learn library. This function randomly divides the data into two subsets: one for training the model and another for evaluating its performance on unseen data.\n",
        "\n",
        "       1.train_test_split randomly shuffles the data before splitting, ensuring\n",
        "         that the training and testing sets are representative of the\n",
        "         entire dataset.\n",
        "       2.A common split is 80% for training and 20% for testing, but this can\n",
        "         be adjusted based on the size of your dataset and the complexity\n",
        "         of your model.\n",
        "       3.The random_state parameter allows you to get the same split each time\n",
        "         you run the code, which is useful for debugging and\n",
        "         comparing different models.\n",
        "       4.The fit() method of the model trains it on the training data, and the\n",
        "         score() method evaluates the model's performance on the testing data.\n",
        "\n"
      ],
      "metadata": {
        "id": "om7Lw8bn68ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que. How do you approach a Machine Learning problem?\n",
        "     \n",
        "  -> The 7 Steps of Machine Learning:\n",
        "\n",
        "     Data Collection. → The quantity & quality of your data dictate\n",
        "     how accurate our model is. ...\n",
        "\n",
        "     Data Preparation. → Wrangle data and prepare it for training. ...\n",
        "\n",
        "     Choose a Model. ...\n",
        "\n",
        "     Train the Model. ...\n",
        "\n",
        "     Evaluate the Model. ...\n",
        "\n",
        "     Parameter Tuning. ...\n",
        "\n",
        "     Make Predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "6eEHgYUny8_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.11 Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "   ->  Exploratory Data Analysis (EDA) before model fitting is crucial for\n",
        "       several reasons. It allows you to thoroughly understand the data's characteristics, identify potential issues, and make informed decisions about data preprocessing, feature engineering, and model selection, ultimately leading to more reliable and accurate model performance.\n",
        "  \n",
        "       1. Understanding Data Characteristics:\n",
        "          EDA helps you understand the underlying patterns, distributions,\n",
        "          and relationships within your data. This includes understanding\n",
        "          data types, identifying missing values, and assessing the range\n",
        "          and distribution of variables.\n",
        "\n",
        "       2. Identifying Data Quality Issues:\n",
        "          EDA helps uncover potential problems like outliers,\n",
        "          inconsistencies, and errors in the data. Addressing these\n",
        "          issues before model building is crucial for ensuring the\n",
        "          model's reliability.\n",
        "\n",
        "       3. Informing Data Preprocessing:\n",
        "          By understanding the data's characteristics, you can make\n",
        "          informed decisions about how to preprocess it, such as\n",
        "          handling missing values, transforming variables, or dealing\n",
        "          with outliers.\n",
        "\n",
        "       4. Guiding Feature Engineering:\n",
        "          EDA can reveal which features are most relevant for your\n",
        "          modeling task, helping you decide which ones to include and how\n",
        "          to engineer them.\n",
        "\n",
        "       5. Facilitating Model Selection:\n",
        "          Understanding the data's distribution and relationships can help\n",
        "          you choose the most appropriate model for your task.\n",
        "\n",
        "       6. Improving Model Performance:\n",
        "          By addressing data quality issues, preprocessing data\n",
        "          appropriately, and selecting the right features and models,\n",
        "          EDA contributes to building more accurate and reliable models.\n",
        "\n",
        "       In essence, EDA is the statistical equivalent of gathering clues\n",
        "       before a test. It allows you to approach model building with a\n",
        "       thorough understanding of the data, increasing the likelihood\n",
        "       of building a robust and effective model."
      ],
      "metadata": {
        "id": "1SCBpDrDxQIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.12 What is correlation?\n",
        "\n",
        "   ->  Correlation is a statistical measure that expresses the extent to which\n",
        "       two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect."
      ],
      "metadata": {
        "id": "1D9Uqc16w4NH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.13 What does negative correlation mean?\n",
        "\n",
        "  ->   A negative correlation means that as one variable increases, the other\n",
        "       variable decreases, and vice versa. This relationship can be visualized as a downward-sloping trend on a graph or scatter plot. In other words, the variables move in opposite directions.\n",
        "\n",
        "       Inverse Relationship:\n",
        "       The variables are inversely related, meaning that an increase in\n",
        "       one leads to a decrease in the other.\n",
        "\n",
        "       Visual Representation:\n",
        "       On a graph, a negative correlation is shown by a best-fit line\n",
        "       that slopes downward from left to right.\n",
        "\n",
        "       Correlation Coefficient:\n",
        "       A negative correlation is indicated by a negative\n",
        "       correlation coefficient (r) value. The closer the r value is to -1,\n",
        "       the stronger the negative correlation.\n",
        "\n",
        "       Not Causation:\n",
        "       Correlation does not equal causation. While a negative\n",
        "       correlation shows a relationship between variables,\n",
        "       it doesn't necessarily mean that one causes the other.\n",
        "\n",
        "       Examples of negative correlation:\n",
        "       The more you eat, the less you work: As you consume more food,\n",
        "       your ability to work decreases due to feeling full and less energetic.\n",
        "\n",
        "       The longer you work, the less free time you have: Spending more\n",
        "       hours at work leaves less time for personal activities.\n",
        "\n",
        "       The colder the weather, the more clothes you wear: As the\n",
        "       temperature drops, you tend to wear more clothing to stay warm.\n",
        "\n",
        "       The cheaper the meal, the more customers who buy it: Lower prices\n",
        "       often lead to increased demand."
      ],
      "metadata": {
        "id": "3Ebkhrqivz6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.14 How can you find correlation between variables in Python?\n",
        "\n",
        "  ->   To find the correlation between variables in Python, several libraries\n",
        "       and methods can be used. Here's how:\n",
        "\n",
        "       Pandas: The corr() method on a Pandas DataFrame calculates\n",
        "       the correlation matrix. By default, it computes the Pearson\n",
        "       correlation coefficient, which measures linear relationships.\n",
        "\n",
        "       Python:\n",
        "       import pandas as pd\n",
        "       df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [5, 4, 3, 2, 1], 'C': [1,\n",
        "            3, 5, 7, 9]})\n",
        "       correlation_matrix = df.corr()\n",
        "       print(correlation_matrix)\n",
        "\n",
        "       NumPy: The corrcoef() function in NumPy calculates the\n",
        "       Pearson correlation coefficient between two arrays.\n",
        "\n",
        "       import numpy as np\n",
        "       array1 = np.array([1, 2, 3, 4, 5])\n",
        "       array2 = np.array([5, 4, 3, 2, 1])\n",
        "       correlation_coefficient = np.corrcoef(array1, array2)[0, 1]\n",
        "       print(correlation_coefficient)\n",
        "\n",
        "       SciPy: The pearsonr(), spearmanr(), and kendalltau() functions in\n",
        "       SciPy calculate Pearson, Spearman, and Kendall\n",
        "       correlation coefficients, respectively. These methods are useful\n",
        "       when dealing with non-linear relationships or non-normally distributed data.\n",
        "\n",
        "       from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "       array1 = np.array([1, 2, 3, 4, 5])\n",
        "       array2 = np.array([2, 4, 1, 3, 5])\n",
        "       pearson_corr, _ = pearsonr(array1, array2)\n",
        "       spearman_corr, _ = spearmanr(array1, array2)\n",
        "       kendall_corr, _ = kendalltau(array1, array2)\n",
        "       print(f\"Pearson: {pearson_corr}, Spearman: {spearman_corr},\n",
        "       Kendall: {kendall_corr}\")\n",
        "\n",
        "       Seaborn: The heatmap() function in Seaborn visualizes the\n",
        "       correlation matrix as a heatmap, which is helpful for\n",
        "       identifying patterns and relationships between variables.\n",
        "\n",
        "       import seaborn as sns\n",
        "       sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "\n",
        "       It's important to remember that correlation does not imply\n",
        "       causation. It only measures the strength and direction of a\n",
        "       linear relationship between variables.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lIiiwe9ts0f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.15 What is causation? Explain difference between correlation and causation\n",
        "       with an example.\n",
        "\n",
        "   ->  Causation means one event directly causes another, while correlation\n",
        "       means two events are related but one doesn't necessarily cause the other. For instance, striking a billiard ball with a cue stick causes the ball to move (causation), but there's a correlation between ice cream sales and crime rates because they both increase during the summer, but ice cream doesn't cause crime.\n",
        "\n",
        "       Causation:\n",
        "       A direct cause-and-effect relationship exists between two events.\n",
        "       One event (the cause) directly leads to another event (the effect).\n",
        "\n",
        "       Example: If you drop a glass, it breaks (the dropping is the cause,\n",
        "       and the breaking is the effect).\n",
        "\n",
        "\n",
        "       Correlation:\n",
        "       Two events are related, meaning they tend to occur together or\n",
        "       change in a similar way.\n",
        "       However, the relationship doesn't necessarily mean one event causes\n",
        "       the other.\n",
        "\n",
        "       Example: There's a correlation between ice cream sales and\n",
        "       pool drownings because both increase in the summer. But eating\n",
        "       ice cream doesn't cause people to drown; the increase in both is due\n",
        "       to warmer weather."
      ],
      "metadata": {
        "id": "L9X-8HsOrCPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.16 What is an Optimizer? What are different types of optimizers? Explain\n",
        "       each with an example.\n",
        "\n",
        "  ->   An optimizer is an algorithm used to adjust a model's parameters (like\n",
        "       weights and biases) during training to minimize a loss function. Different types of optimizers, like Gradient Descent, Stochastic Gradient Descent, and Adam, employ different strategies to achieve this.\n",
        "\n",
        "       Types of Optimizers and Examples:\n",
        "       1. Gradient Descent:\n",
        "          Mechanism: Iteratively adjusts parameters by moving in the\n",
        "          direction of the negative gradient of the loss function.\n",
        "\n",
        "          Example: Imagine a ball rolling downhill. Gradient descent is\n",
        "          like the ball finding its way to the lowest point\n",
        "          (the minimum of the loss function).\n",
        "\n",
        "       2. Stochastic Gradient Descent (SGD):\n",
        "          Mechanism: Updates parameters based on the gradient calculated\n",
        "          from a single training example (or a small batch) at each iteration.\n",
        "\n",
        "          Example: If you have a large dataset, SGD can be faster than\n",
        "          Batch Gradient Descent because it doesn't wait for the\n",
        "          entire dataset to be processed before updating.\n",
        "\n",
        "        3. Adam (Adaptive Moment Estimation):\n",
        "           Mechanism: Adjusts learning rate for each parameter based on\n",
        "           the first and second moments of the gradients, making it adaptive.\n",
        "\n",
        "           Example: In a complex model with many parameters, Adam can find\n",
        "           a good balance between exploration and exploitation, leading\n",
        "           to faster convergence and potentially higher accuracy.\n",
        "\n",
        "        4. AdaGrad (Adaptive Gradient):\n",
        "           Mechanism: Adapts learning rate for each parameter based on\n",
        "           the historical gradient information.\n",
        "\n",
        "           Example: Useful in scenarios with sparse data, where\n",
        "           some parameters might not have frequent updates, according to\n",
        "           a blog post.\n",
        "\n",
        "        5. RMSprop (Root Mean Square Propagation):\n",
        "           Mechanism: Similar to AdaGrad but uses a moving average of\n",
        "           squared gradients, helping to avoid oscillations and improve stability.\n",
        "           Example: Effective in situations where the loss landscape\n",
        "           is irregular and gradients fluctuate significantly."
      ],
      "metadata": {
        "id": "HFP_BqffoFgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.17 What is sklearn.linear_model ?\n",
        "\n",
        "   ->  sklearn.linear_model is a module in the scikit-learn (sklearn) library\n",
        "       that implements various linear models for machine learning tasks. These models assume a linear relationship between the input features and the target variable. The module includes algorithms for regression, classification, and multi-task learning.\n",
        "\n",
        "       Some of the commonly used classes and functions in sklearn.\n",
        "       linear_model are:\n",
        "\n",
        "       LinearRegression: Ordinary least squares linear regression.\n",
        "\n",
        "       Ridge: Linear least squares with L2 regularization.\n",
        "\n",
        "       Lasso: Linear model with L1 regularization (Lasso).\n",
        "\n",
        "       ElasticNet: Linear regression with combined L1 and L2 regularization.\n",
        "\n",
        "       LogisticRegression: Logistic regression for binary and\n",
        "       multiclass classification.\n",
        "\n",
        "       SGDRegressor: Linear model fitted by minimizing a regularized\n",
        "       empirical loss with SGD.\n",
        "       \n",
        "       SGDClassifier: Linear classifiers (SVM, logistic regression, etc.)\n",
        "       with SGD training.\n",
        "\n",
        "       These models offer flexibility in handling different types of data\n",
        "       and addressing various machine learning problems. They are widely\n",
        "       used due to their simplicity, interpretability, and\n",
        "       efficiency, especially when dealing with linearly separable data\n",
        "       or when used as a baseline for more complex models."
      ],
      "metadata": {
        "id": "3VO5osmfmoFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.18 What does model.fit() do? What arguments must be given?\n",
        "\n",
        "   ->  The model.fit() function in machine learning frameworks like TensorFlow\n",
        "       and scikit-learn trains a model on a given dataset. It adjusts the model's internal parameters to minimize a loss function, using optimization techniques such as gradient descent.\n",
        "\n",
        "       The required arguments for model.fit() depend on whether\n",
        "       it's a supervised or unsupervised learning task.\n",
        "\n",
        "       For supervised learning, it generally takes two arguments:\n",
        "       x: The training data features.\n",
        "       y: The target labels corresponding to the training data.\n",
        "\n",
        "       For unsupervised learning, it usually takes only one argument:\n",
        "       x: The training data.\n",
        "       Optional arguments can be provided to control the training\n",
        "       process, including:\n",
        "\n",
        "       batch_size: The number of samples processed before updating the\n",
        "       model's parameters.\n",
        "\n",
        "       epochs: The number of times the entire dataset is iterated over\n",
        "       during training.\n",
        "\n",
        "       validation_data: Data used to evaluate the model's performance\n",
        "       during training.\n",
        "\n",
        "       validation_split: Fraction of the training data to be used\n",
        "       as validation data.\n",
        "\n",
        "       callbacks: Actions performed at certain stages of the training\n",
        "       process, like saving the model or adjusting the learning rate.\n",
        "\n",
        "       verbose: Controls the amount of output displayed during training."
      ],
      "metadata": {
        "id": "UYcr0j8ylkdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.19 What does model.predict() do? What arguments must be given?\n",
        "\n",
        "   ->  Purpose : model. predict() is used to generate predictions from the\n",
        "       trained model based on new input data. It does not require true labels and does not compute any metrics.\n",
        "\n",
        "       predict() : given a trained model, predict the label of a new set\n",
        "       of data. This method accepts one argument, the new data X_new\n",
        "       (e.g. model. predict(X_new) ), and returns the learned label for\n",
        "       each object in the array."
      ],
      "metadata": {
        "id": "e7zyq--RlDLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.20 What are continuous and categorical variables?\n",
        "\n",
        "   ->  In statistics, continuous variables represent data that can take any\n",
        "       value within a given range, while categorical variables represent data that fall into distinct categories or groups. Continuous variables are typically numeric and quantifiable, while categorical variables are non-numeric and descriptive.\n",
        "\n",
        "       Continuous Variables:\n",
        "       Can take any value within a specific range.\n",
        "       Examples include height, weight, temperature, and time.\n",
        "       Often measured on scales like interval or ratio.\n",
        "       Can be further categorized as discrete or continuous.\n",
        "\n",
        "       Categorical Variables:\n",
        "       Represent data that can be placed into distinct groups or categories.Examples include gender, eye color, or type of car.\n",
        "       Also known as qualitative or discrete variables.\n",
        "       Can be further categorized as nominal, ordinal, or dichotomous."
      ],
      "metadata": {
        "id": "ExTxjOm-kc2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.21 What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "   ->  Feature scaling, also known as normalization or standardization, is a\n",
        "       preprocessing step in machine learning that transforms the values of numerical features to a standard scale or range. This ensures that all features contribute equally to the learning process, preventing features with larger values from disproportionately influencing the model's performance.\n",
        "\n",
        "       Equal Contribution:\n",
        "       By scaling features, you ensure that each feature has a similar\n",
        "       impact on the model, even if they have different ranges or units.\n",
        "\n",
        "       Improved Model Performance:\n",
        "       Scaling can help algorithms that rely on distance calculations\n",
        "       (like k-nearest neighbors) or gradient descent (like linear\n",
        "       regression and neural networks) converge faster and more accurately.\n",
        "\n",
        "       Reduced Numerical Instability:\n",
        "       Scaling can prevent issues caused by large feature values, which\n",
        "       can lead to numerical instability in some algorithms.\n",
        "\n",
        "       Enhanced Interpretability:\n",
        "       Scaled features can be easier to interpret and compare.\n",
        "\n",
        "       Prevent Overfitting:\n",
        "       Scaling can help prevent models from overfitting by making it\n",
        "       harder for them to learn irrelevant patterns in the data.\n",
        "\n",
        "       Common Scaling Methods:\n",
        "\n",
        "       Normalization (Min-Max Scaling): Transforms values to a specific\n",
        "       range, often between 0 and 1.\n",
        "\n",
        "       Standardization: Transforms features to have a mean of 0 and a\n",
        "       standard deviation of 1."
      ],
      "metadata": {
        "id": "HJUoZVStjo3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.22 How do we perform scaling in Python?\n",
        "\n",
        "   ->  Scaling data in Python involves transforming numerical features to a\n",
        "       similar scale. This is crucial for machine learning algorithms sensitive to feature magnitude. Common scaling methods include:\n",
        "\n",
        "       1.Min-Max Scaling (Normalization): It scales data to a range between 0\n",
        "         and 1.\n",
        "\n",
        "         from sklearn.preprocessing import MinMaxScaler\n",
        "         data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
        "         scaler = MinMaxScaler()\n",
        "         scaled_data = scaler.fit_transform(data)\n",
        "         print(scaled_data)\n",
        "         # Expected Output\n",
        "         # [[0.  0.  ]\n",
        "         # [0.25 0.25]\n",
        "         # [0.5  0.5 ]\n",
        "         # [1.  1.  ]]\n",
        "\n",
        "        2.Standard Scaling (Standardization): It scales data to have a mean of\n",
        "          0 and a standard deviation of 1.\n",
        "\n",
        "          from sklearn.preprocessing import StandardScaler\n",
        "          data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
        "          scaler = StandardScaler()\n",
        "          scaled_data = scaler.fit_transform(data)\n",
        "          print(scaled_data)\n",
        "          # Expected Output\n",
        "          # [[-1.18321596 -1.06066017]\n",
        "          # [-0.50713967 -0.44277507]\n",
        "          # [ 0.16904656  0.17611003]\n",
        "          # [ 1.52130907  1.32732521]]\n",
        "\n",
        "        3.Robust Scaling: It is less sensitive to outliers, using the median\n",
        "          and interquartile range for scaling.\n",
        "\n",
        "          from sklearn.preprocessing import RobustScaler\n",
        "          data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
        "          scaler = RobustScaler()\n",
        "          scaled_data = scaler.fit_transform(data)\n",
        "          print(scaled_data)\n",
        "          # Expected Output\n",
        "          # [[-1.  -1. ]\n",
        "          # [-0.5 -0.4]\n",
        "          # [ 0.   0. ]\n",
        "          # [ 1.   1.6]]\n",
        "\n",
        "        4.Power Transformer Scaling: It applies a power transformation to make\n",
        "          data more Gaussian-like, addressing issues related to variance\n",
        "          and skewness.\n",
        "\n",
        "          from sklearn.preprocessing import PowerTransformer\n",
        "          data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
        "          scaler = PowerTransformer()\n",
        "          scaled_data = scaler.fit_transform(data)\n",
        "          print(scaled_data)\n",
        "          # Expected Output\n",
        "          # [[-1.28351368 -1.31820193]\n",
        "          # [-0.59888587 -0.57452013]\n",
        "          # [ 0.05362751  0.13297776]\n",
        "          # [ 1.82877204  1.7597443 ]]"
      ],
      "metadata": {
        "id": "hRJyXNJSgAUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.23 What is sklearn.preprocessing?\n",
        "\n",
        "   ->  The sklearn.preprocessing module in scikit-learn provides a set of  \n",
        "       tools for transforming raw data into a format suitable for machine learning models. It includes functions for:\n",
        "\n",
        "       Scaling and normalization:\n",
        "       Adjusting the range of feature values to prevent certain features\n",
        "       from dominating others. Methods include StandardScaler,\n",
        "       MinMaxScaler,  Normalizer, and RobustScaler.\n",
        "\n",
        "       Encoding categorical variables:\n",
        "       Converting categorical data into numerical representations that\n",
        "       machine learning algorithms can process. Techniques\n",
        "       include OneHotEncoder, OrdinalEncoder, and LabelEncoder.\n",
        "\n",
        "       Imputation:\n",
        "       Handling missing values by replacing them with estimated values\n",
        "       using methods like SimpleImputer or KNNImputer.\n",
        "\n",
        "       Generating polynomial features:\n",
        "       Creating new features by raising existing features to certain powers\n",
        "       or including interaction terms, using PolynomialFeatures.\n",
        "\n",
        "       Non-linear transformations:\n",
        "       Applying non-linear transformations to features, such as quantile\n",
        "       or power transforms, using QuantileTransformer or PowerTransformer.\n",
        "\n",
        "       Data preprocessing with sklearn.preprocessing is crucial for\n",
        "       improving the performance and accuracy of machine learning models\n",
        "       by ensuring that the data is clean, consistent, and in a\n",
        "       suitable format for training."
      ],
      "metadata": {
        "id": "o3ogI30bduZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.24 How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "   ->  In Python, data is typically split into training and testing sets using\n",
        "       the train_test_split function from the sklearn.model_selection module. This function randomly divides the dataset into two subsets: one for training the machine learning model, and another for evaluating its performance on unseen data.\n",
        "\n",
        "       1. Prepare Your Data:\n",
        "          Ensure your data is in a format that sklearn can handle, often\n",
        "          as a Pandas DataFrame or NumPy array.\n",
        "          Separate your data into features (independent variables) and\n",
        "          the target variable (dependent variable).\n",
        "\n",
        "        2. Import the Function:\n",
        "           from sklearn.model_selection import train_test_split\n",
        "\n",
        "        3. Use train_test_split:\n",
        "           X_train, X_test, y_train, y_test = train_test_split\n",
        "           (X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "          X represents the features (independent variables) of your dataset.\n",
        "          y represents the target variable (dependent variable).\n",
        "\n",
        "          test_size=0.2 specifies that 20% of the data will be allocated\n",
        "          to the testing set. You can adjust this ratio as needed.\n",
        "\n",
        "          random_state=42 sets a random seed for reproducibility.\n",
        "          This ensures that the same split is obtained each time you run\n",
        "          the code, which is helpful for debugging and comparing results.\n",
        "\n",
        "        4. Understanding the Output:\n",
        "           X_train: Features for the training set.\n",
        "           X_test: Features for the testing set.\n",
        "           y_train: Target variable for the training set.\n",
        "           y_test: Target variable for the testing set.\n",
        "\n",
        "          Example:\n",
        "          Let's say you have a Pandas DataFrame df with features in\n",
        "          columns ['feature1', 'feature2'] and the target variable in\n",
        "          column ['target']:\n",
        "\n"
      ],
      "metadata": {
        "id": "gBKf5lJYca6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que.25 Explain data encoding?\n",
        "\n",
        "   ->  Data encoding is the process of converting information from one format\n",
        "       to another, often for transmission, storage, or processing by computers. It involves assigning a unique representation (like a binary code or a character set) to data, ensuring it can be understood by the system or algorithm receiving it.\n",
        "       Encoding categorical variables is crucial for machine learning algorithms that require numerical input.\n",
        "       One-hot encoding is a common technique for handling categorical features in machine learning models.\n",
        "\n",
        "       Types of Data Encoding:\n",
        "\n",
        "       Character Encoding: Assigns numerical values to characters,\n",
        "       like ASCII or Unicode.\n",
        "\n",
        "       Binary Encoding: Represents data using only 0s and 1s.\n",
        "\n",
        "       One-Hot Encoding: Transforms categorical data into a binary\n",
        "       matrix, where each unique category is represented by a unique column.\n",
        "\n",
        "       Label Encoding: Assigns a numerical label to each category, but may\n",
        "       not preserve the order of the categories.\n",
        "\n",
        "       Ordinal Encoding: Assigns numerical labels to categories in a\n",
        "       specific order, preserving the ordinal relationship.\n",
        "\n",
        "       Feature Scaling: Scales numerical features to a specific range\n",
        "       (e.g., 0 to 1).\n",
        "\n",
        "       Examples of Data Encoding:\n",
        "\n",
        "       Unicode: A character encoding standard that represents most of\n",
        "       the world's characters.\n",
        "\n",
        "       JSON: A lightweight data-interchange format often used for web applications.\n",
        "\n",
        "       Compression algorithms: Like gzip or zlib, which encode data to\n",
        "       reduce storage space.\n",
        "\n",
        "       Encryption algorithms: Like AES or RSA, which encode data to protect\n",
        "       it from unauthorized access.\n"
      ],
      "metadata": {
        "id": "0NQskssvbCiM"
      }
    }
  ]
}